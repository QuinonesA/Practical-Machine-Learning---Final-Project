---
title: "Practical Machine Learning - Final Course Project"
author: "Alejandra Qui√±ones"
date: "07 May 2020"
output: html_document
---
###Introduction
This document is being submitted as part of the Final Project of the Coursera Practical Machine Learning Course. Using data from accelerometers on the belt, forearm, arm and dumbell (cite on README.md), a model was created to predict if barbell lifts were performed correctly or incorrectly. Class A corresponds to a correct execution, while classes B to E correspond to common mistakes. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Data and package loading

```{r, warning=F, message=FALSE}
library(caret); library(corrplot); library(randomForest); library(pROC)
     
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```

### Exploring the data

```{r}
dim(training); dim(testing)
```
At first glance, both datasets have 160 variables. The training set is a large dataset with 19622 observations while the testing set holds 20. Next, a function was created to show the name, the amount of complete cases and the class of each variable.  

```{r}
showna <- function(df){
     nadf <- data.frame(colnames(df))
     nadf$complete <- NA
     nadf$colclass <- NA
     for (i in 1:ncol(df)){
          na <- sum(complete.cases(df[,i]))
          nadf[i,][,2] <- na
          nadf[i,][,3] <- class(df[,i])
     }
     return(nadf)
}

sumtrain <- showna(training)
head(sumtrain, 20)
sumtest <- showna(testing)
head(sumtest, 20)
```

At first glance, both sets have many variables with a considerable amount of NAs, and some wrongly classified. To prevent future issues, the columns 12 to 159 will be reclassified as numeric. In addition, since both data frames hold the same variables, their levels were made equal.

```{r, warning=F}
#Fixing variable classification
     
for (i in 12:159){
     training[,i] <- as.numeric(as.character(training[,i]))
}
rm(i)
     
for (i in 12:159){
     testing[,i] <- as.numeric(as.character(testing[,i]))
}
rm(i)
     
#Adjusting levels
for (i in 1:ncol(testing)){levels(testing[,i]) <- levels(training[,i])}
rm(i)

```

###Data preprocessing
Once classification errors were fixed, training set preprocessing was carried out. Since multiple variables held a significant amount of missing values, imputing them could lead to overfitting. Thus, variables with less than 70% of complete cases were removed. This left the new training set with 60 variables.

```{r}
trainclean <- training
trainclean <- training[, colSums(is.na(training)) <= nrow(training)*0.3]

```

A correlation analysis was carried out. 
```{r}
cormat <- cor(trainclean[,7:59])
corrplot(cormat, method="color", type="lower", order="AOE", tl.cex = .3, tl.col = "black", diag = F)

```

A few variables showed a high level of correlation. Preprocessing by PCA could be used to solve this, but since a lot of variables were already dismissed, this could be bypassed by using a robust model. 


Next, the preProcess function of the caret package was used to detect and dismiss variables with near zero variance. Variables unrelated to the accelerometers were removed as well. Once this was done, the training set was ready for the modeling algorithms.
```{r}
preobj <- preProcess(trainclean, method="zv")
     
#New values
pptrain <- predict(preobj, trainclean)
pptrain$classe <- trainclean$classe 


```


###Model fitting

##Training set spliting
Before creating the prediction models, the training set was split into a building set, based on which the models were created, and a validation set, where the models were then tested. 

```{r}
set.seed(3333)
inBuild <- createDataPartition(y=pptrain$classe, p=0.7, list=F)
validation <- pptrain[-inBuild,]
build <- pptrain[inBuild,] 

```

Next up, three modeling algorithms that could predict multiclass outcomes with high accuracy were used to fit two models. Traincontrol arguments were set with 3-fold cross-validation. All were then tested on the validation set. 


##Decision Tree Model
```{r}
modtree <- train(classe ~., data=build, method="rpart", trControl = trainControl(method="cv", number=3))
modtree

#Predicting outcome in validation set     
predtree <- predict(modtree, newdata = validation)
confusionMatrix(predtree, validation$classe)

acctree <- confusionMatrix(predtree, validation$classe)$overall["Accuracy"]

```


##Random Forest Model
For this model, an mtry value of 2 was considered based on a previous random forest model in which the highest accuracy was gotten with said mtry value. Specifying this parameter will make the processing much faster.  

```{r}
modrf <- randomForest(classe ~., data=build, mtry=2, importance=TRUE, trControl = trainControl(method="cv"), number = 3)
modrf

#Predicting outcome in validation set     
predrf <- predict(modrf, newdata = validation)
cm <- confusionMatrix(predrf, validation$classe)

accrf <- confusionMatrix(predrf, validation$classe)$overall["Accuracy"]

```

Based on their accuracy on the validation set, the random forest model is the best one out of both models.
```{r}
t(data.frame(acctree, accrf))

```

As further analysis of the random forest model, the error rate was plotted against the number of trees and a multiclass ROC curve was calculated.
```{r, warning=F, message=FALSE}

#Ploting the model
plot(modrf, main="Random forest model")
legend("right", cex =1, legend=colnames(modrf$err.rate), lty=c(1,2,3), col=c(1,2,3))


#Calculating ROC curve
roc <- multiclass.roc(as.numeric(validation$classe), as.numeric(predrf))
     
roc

```
The area under the curve calculated for the random forest is above 0.80, which means it is a good predictor.

#Predictions of the random forest model on the validation set
```{r}
plot(cm$table, col=cm$byClass, main=paste("Random Forest Model    ", "Accuracy:", round(cm$overall["Accuracy"], 4)))

```

###Predictions on the test set
In order to predict the classes in the 20 observations on the testing set, the same preprocessing as in the training set was carried out: Variables with missing values and/or near zero variance were dismissed.
```{r}
testclean <- testing
testclean <- testing[, colSums(is.na(testing)) == 0]
testclean$problem_id <- NULL
     
preot <- preProcess(testclean, method="zv")
pptest <- predict(preot, testclean)
pptest$classe <- as.factor(NA)

```


Finally, as the best predictor, the random forest model was used to predict the class in the 20 observations included in the testing set. All were then plotted.  
```{r}
predtest <- predict(modrf, newdata = pptest)
     
     
predictions <- data.frame(predtest)
     
predictions

qplot(x=seq_along(1:20), y=predictions$predtest, ylab="Classes", xlab="Problem_ID", colour=predictions$predtest, show.legend = FALSE, size=3, main="Predicted class in test set") + theme(plot.title = element_text(hjust = 0.5))     
```